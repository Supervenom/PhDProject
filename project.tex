\documentclass[english]{scrartcl}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{microtype}
\addtokomafont{disposition}{\rmfamily}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}

\begin{document}
\title{Research project proposal}
\subtitle{Computability, complexity and efficiency of optimal reductions in functional programming languages}
\author{Gabriele Vanoni}
\date{}
\maketitle
\section{State of the art}
There are different possible \emph{strategies} when you have to evaluate expressions. Some are better than others, in the sense that bring you to the result in a lower number of steps. Since programs in pure functional languages are essentially expressions, the problem of defining good strategies is particularly interesting. Finding \emph{minimal} strategies, i.e. strategies that minimize the number of steps, seems even more interesting. However this problem has been proven undecidable for the $\lambda$-calculus \cite[Section~13.5]{barendregt_lambda_1984}, \emph{the} paradigmatic pure functional language. Morover the result was strengthened in \cite{kathail_optimal_1990}, where he showed that the minimal strategy is non recursive also modulo an arbitrary multiplicative constant. Though researchers have not abandoned the field and since ``optimality and computability cannot be both achieved at the same time, the trick consists in a change of granularity'' \cite{terese_term_2003}. Around 40 years ago Jean-Jacques Lévy introduced the notion of \emph{optimal reduction}, where optimal roughly means without duplication of reduction work and without any useless computation \cite{levy_reductions_1978}. He proved that such a strategy exists and is recursive but he wasn't able to provide an actual implementation by means of real data structures. After more than 10 years Lamping \cite{lamping_algorithm_1990} and Kathail \cite{kathail_optimal_1990} indipendently proposed the first algorithms to actually implement optimal reductions in the sense of Lévy. Lamping's algorithm implemented optimal reductions by means of the so called \emph{sharing graphs} with some \emph{rewriting rules} sound with respect to Lévy specification. The algorithm captured the interest of the community, that began to work trying to simplify and better understand it. The research on this topic took different directions.
\paragraph{Mathematical foundations of optimal reductions.}Lamping proposed his algorithm for optimal reductions in the same years Girard published his works on \emph{Linear Logic} \cite{girard_linear_1987} and \emph{Geometry of Interaction} \cite{girard_geometry_1989}. Soon it became clear that there was a strong connection between these two fields \cite{gonthier_geometry_1992,gonthier_linear_1992}. This strong connection allowed to achieve at least two important goals. Firstly Geometry of Interaction provided an elegant mathematical framework suitable to better understand optimal reductions and to improve and simplify Lamping's method. Secondly graphs similar to Lamping's were a concrete representation of the Geometry of Interaction. Moreover Asperti rephrased the correspondence between Linear Logic and optimal reductions in a categorical perspective in \cite{asperti_linear_1995} going another step further in the understanding of the subject, and further simplifying the algorithm.
\paragraph{Cost models and efficiency of the $\lambda$-calculus.}Since the birth of computational complexity theory, Turing Machines were tipically used to define complexity classes \cite{hartmanis_computational_1965}. This is reasonable since this abstract model of computation reflects quite well actual hardware. However higher-order functional programming languages, such as Haskell or Caml, subsume a model of computation more akin to the $\lambda$-calculus than to the Turing Machines. Thus the study of the computational complexity of programs written in these languages is not easy using the standard machinery of complexity theory. In this way, it becomes interesting to study the computational complexity of the $\beta$-rule, the only computational mechanism of the $\lambda$-calculus. In particular a single $\beta$-reduction step can correspond to many moves of a Turing Machine implementing it, i.e. $\beta$-reduction is in general not \emph{atomic}. A recent result \cite{dal_lago_leftmost-outermost_2016} however proved that the $\lambda$-calculus equipped with a leftmost-outermost evaluation strategy can be simulated by a Turing Machine with only a polynomial overhead in time, i.e. the number of $\beta$-steps to normal form \emph{is} a reasonable measure of the complexity of a $\lambda$-term. But what about efficiency? Leftmost-outermost in general is not efficient since it reduces redexes before evaluating their arguments (e.g. $(\lambda x.xx)(\mathbf{I}x)$ ), but on the other hand innermost strategies are not normalizing and can do useless reduction work (e.g. $(\lambda x.z)\bm{\Omega}$ ). And optimal reductions in the sense of Lévy are efficient? There is no definite answer to this question. Optimal reductions in fact are \emph{optimal} with respect to the number of parallel $\beta$-reductions, which however was proved not to be a reasonable cost model \cite{lawall_optimality_1996, asperti_parallel_2001, asperti_optimal_2004}. But this tell us nothing about the efficiency. The only results in the literature are partial, i.e. restricted to some classes of $\lambda$-terms, and all of them are positive. In \cite{baillot_light_2011} the authors proved that $\lambda$-terms with known bounded complexity, polynomial and elementary time, were normalized via sharing graphs with a cost respectively polynomial and elementary. More recently in \cite{guerrini_is_2017} this result has been generalized to arbitrary reductions and a clear bound on the sharing overhead was given. \cite{asperti_about_2017} is an updated short survey on the subject.
\paragraph{Optimal implementation of functional programming languages.}Soon after the publication of Lamping's algorithm researchers tried to extend his method to real functional programming languages, since coding everything as $\lambda$-terms is clearly infeasible. \emph{Interaction systems} \cite{asperti_interaction_1994, asperti_interaction_1996} are the technical tool developed for this reason and allowed Lamping's algorithm to work with a larger and more flexible calculus featuring for example inductive data types, conditionals and recursion. The \emph{Bologna Optimal Higher-Order Machine} (BOHM) was the first prototype implementation of these ideas \cite{asperti_bologna_1996}. It was written in C and was an interpreter for a sugared $\lambda$-calculus enriched with booleans, integers, lists and basic operations on these data types. Benchmarks in \cite{asperti_optimal_1998} showed that BOHM outperformed both Caml Light and Haskell on pure $\lambda$-terms and was comparable on typical symbolic computations. \cite{asperti_optimal_1998} contains a self-contained treatment of the whole material behind optimal reductions, from the mathematical theory to the more practical aspects of the implementation, such as garbage collection.
\section{Project description}
\section{Expected results}
\bibliographystyle{alpha}
\bibliography{project}
\end{document}