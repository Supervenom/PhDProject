\documentclass[english]{scrartcl}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{microtype}
\addtokomafont{disposition}{\rmfamily}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{tabularx}

\begin{document}
\title{Research Project Proposal}
\subtitle{Computability, Complexity and Efficiency of Optimal Reductions in Functional Programming Languages}
\author{Gabriele Vanoni}
\date{}
\maketitle
\section{State of the Art}
There are different possible \emph{strategies} you can follow to evaluate expressions. Some are better than others, and bring you to the result in a lower number of steps. Since programs in pure functional languages are essentially expressions, the problem of defining good strategies is particularly interesting. Finding \emph{minimal} strategies, i.e. strategies that minimize the number of steps, seems even more interesting. However the problem of picking the redex leading to the reduction sequence of least length has been proven undecidable for the $\lambda$-calculus \cite[Section~13.5]{barendregt_lambda_1984}, \emph{the} paradigmatic pure functional language. Morover the result was strengthened by Kathail \cite[Appendix]{kathail_optimal_1990}, who showed that the minimal strategy is non recursive also modulo an arbitrary multiplicative constant. Thus, optimality should be achieved in another way and since ``optimality and computability cannot be both achieved at the same time, the trick consists in a change of granularity'' \cite{terese_term_2003}. Around 40 years ago, indeed, Jean-Jacques Lévy introduced the notion of \emph{optimal reduction}, where optimal roughly means without duplication of reduction work and without any useless computation \cite{levy_reductions_1978}. He proved that such a strategy exists and is recursive but he was not able to provide an actual implementation. After more than 10 years Lamping \cite{lamping_algorithm_1990} and Kathail \cite{kathail_optimal_1990} indipendently proposed the first algorithms to actually implement optimal reductions in the sense of Lévy. Lamping's algorithm implemented optimal reductions by means of the so-called \emph{sharing graphs} with some \emph{rewriting rules} sound with respect to Lévy specification. The algorithm captured the interest of the community, that began to work trying to simplify and better understand it. The research on this topic took different directions.
\paragraph{Mathematical Foundations of Optimal Reductions.}Lamping proposed his algorithm for optimal reductions in the same years Girard published his works on \emph{Linear Logic} \cite{girard_linear_1987} and \emph{Geometry of Interaction} \cite{girard_geometry_1989}. Soon it became clear that there was a strong connection between these two fields \cite{gonthier_geometry_1992,gonthier_linear_1992}. This strong connection allowed to achieve at least two important goals. Firstly, Geometry of Interaction provided an elegant mathematical framework suitable to better understand optimal reductions and to improve and simplify Lamping's method. Secondly, graphs similar to Lamping's were a concrete representation of the Geometry of Interaction. Moreover Asperti rephrased the correspondence between Linear Logic and optimal reductions in a categorical perspective in \cite{asperti_linear_1995} going another step further in the understanding of the subject, and further simplifying the algorithm.
\paragraph{Cost Models and Efficiency of the $\lambda$-calculus.}Since the birth of computational complexity theory, Turing Machines were tipically used to define complexity classes \cite{hartmanis_computational_1965}. This is reasonable, since this abstract model of computation reflects quite well actual hardware. However, higher-order functional programming languages, such as \texttt{Haskell} or \texttt{Caml}, subsume a model of computation more akin to the $\lambda$-calculus than to the Turing Machines. Thus, the study of the computational complexity of programs written in these languages is not easy using the standard machinery of complexity theory. This way, it becomes interesting to study the computational complexity of the $\beta$-rule itself, the only computational mechanism of the $\lambda$-calculus. In particular a single $\beta$-reduction step can correspond to many moves of a Turing Machine implementing it, i.e. $\beta$-reduction is in general not \emph{atomic}. A recent result \cite{accattoli_leftmost-outermost_2016}, however, proved that the $\lambda$-calculus equipped with a leftmost-outermost evaluation strategy can be simulated by a Turing Machine with only a polynomial overhead in time, i.e. the number of $\beta$-steps to normal form \emph{is} a reasonable measure of the complexity of a $\lambda$-term. But how about efficiency? Leftmost-outermost is in general not efficient since it reduces redexes before evaluating their arguments (e.g. in terms like $(\lambda x.xx)(\mathbf{I}x)$ ), but on the other hand innermost strategies are not normalizing and can do useless reduction work (e.g. in terms like $(\lambda x.z)\bm{\Omega}$ ). Are optimal reductions in the sense of Lévy efficient? There is no definite answer to this question. Optimal reductions in fact are \emph{optimal} with respect to the number of parallel $\beta$-reductions, which however was proved \emph{not} to be a reasonable cost model \cite{lawall_optimality_1996, asperti_parallel_2001, asperti_optimal_2004}. But this tell us nothing about efficiency. The only results in the literature are partial, i.e. restricted to some classes of $\lambda$-terms, and all of them are positive. In \cite{baillot_light_2011} the authors proved that $\lambda$-terms with known bounded complexity, polynomial and elementary time, were normalized via sharing graphs with a cost respectively polynomial and elementary. More recently in \cite{guerrini_is_2017} this result has been generalized to arbitrary reductions and a clear bound on the sharing overhead was given. \cite{asperti_about_2017} is an updated short survey on the subject.
\paragraph{Optimal Implementation of Functional Programming Languages.}Soon after the publication of Lamping's algorithm, researchers tried to extend his method to real functional programming languages, since coding everything as $\lambda$-terms is clearly infeasible. \emph{Interaction systems} \cite{asperti_interaction_1994, asperti_interaction_1996} are the technical tool developed for this reason and allowed Lamping's algorithm to work with a larger and more flexible calculus featuring, for example inductive data types, conditionals and recursion. The \emph{Bologna Optimal Higher-Order Machine} (BOHM) was the first prototype implementation of these ideas \cite{asperti_bologna_1996}. It was written in \texttt{C} and was an interpreter for a sugared $\lambda$-calculus enriched with booleans, integers, lists and basic operations on these data types. Benchmarks in \cite{asperti_optimal_1998} showed that BOHM outperformed both \texttt{Caml Light} and \texttt{Haskell} on pure $\lambda$-terms and was comparable on typical symbolic computations. \cite{asperti_optimal_1998} contains a self-contained treatment of the whole material behind optimal reductions, from the mathematical theory to the more practical aspects of the implementation, such as garbage collection.
\paragraph{Geometry of Interaction and Optimal Implementations.}Besides providing a better understanding of Lamping's technique (as pointed above), Geometry of Interaction yields a tool for implementing functional programming languages. Mackie \cite{mackie_geometry_1995} proposed a concrete implementation for \texttt{PCF} that compiles directly to assembly language. His implementation is very parsimonious in space and not aimed at time efficiency. However he proposed also an optimization where space is traded off for time. The trade-off between time and space efficiency is the core of a recent work about abstract machines implementing the $\lambda$-calculus through the Geometry of Interaction \cite{muroya_dynamic_2018}, where different evaluation strategies provide different consumption of time and space. Yet the investigation does not cover the optimal reduction strategy. The link was shown in \cite{danos_reversible_1999}, where different abstract machines (one of which based on the Geometry of Interaction) implementing the same strategy were compared, pointing out how machines could be more efficient in time at the cost of consuming more space. Optimal machines \emph{à la} Lévy can be obtained from one of these machines further consuming more space through a memoization process, but no claims about time efficiency were reported.
\section{Project Description}
The project will consist in a complete investigation over computability, complexity and efficiency of optimal reductions in functional programming languages, from a theoretical and practical perspective. The state of the art has been stalling for years although many interesting problems were still open. Hence there is space to obtain new and interesting results. The Dipartimento di Informatica - Scienza e Ingegneria of Università di Bologna is the perfect environment for such a project. In fact some of its members, namely Andrea Asperti, Ugo Dal Lago, Cosimo Laneve, Simone Martini, are among the main contributors in the field of optimal reduction. The scientific collaboration with the INRIA research institute in the scope of Focus Reasearch Team also provides a good opportunity e.g. for the mandatory visiting period abroad. The project is composed of different interconnected aspects.
\paragraph{Computability of Minimal Reduction Strategies.}Barendregt's proof \cite[Section~13.5]{barendregt_lambda_1984} states that there is no recursive minimal reduction strategy. He uses an \emph{ad-hoc} argument difficult to generalize. A first generalization appeared in \cite[Appendix]{kathail_optimal_1990} where the impossibility result was extended to minimal strategies modulo an arbitrary multiplicative constant. We want to carefully study these proofs because we think the result can be further extended. In fact we think that no recursive minimal strategy exists modulo a polynomial, elementary or maybe any computable function.
\paragraph{Complexity of Optimal Reduction Strategies.}We wish to extend the few results available in the literature \cite{baillot_light_2011,guerrini_is_2017} to arbitrary $\lambda$-terms. This implies the study of the complexity of the \emph{bookkeping} part of Lamping's algorithm, an interesting theme in itself. In particular it is conjectured that it only adds a polynomial overhead to the reduction cost, but no proof is available. A good avenue seems to be the study of an abstract machine performing optimal reductions as suggested in \cite{danos_reversible_1999}. 
\paragraph{Resource-Aware Implementation of Functional Programming Languages.}It is well-known that the memory access is the bottleneck on performances of modern computers \cite{hennessy_computer_2011}. \emph{Caching} is one of the main mechanisms used in computer architectures to overcome this issue. Caches are very fast with respect to main memory but, still, very small. Implementations of functional programming languages should take into account this aspect. Optimal implementations in particular need a great amount of memory to mantain all data structures of Lamping's algorithm. What if the speedup due to the efficiency of reductions is lost because of cache misses? We will try to figure this out. Moreover what if the reduction strategy is changed during the execution of a program (statically at \emph{compile time} or dynamically at \emph{runtime})? For example, once the cache (or the cache hierarchy) is filled, one could change to a reduction strategy space constrained, such as the one provided by the Geometry of Interaction interpretation of the $\lambda$-calculus. Would this \emph{fully-in-cache} implementation be more efficient on real machines than the pure optimal reduction?
\subsection*{Time Schedule}All the different aspects of this project are interconnected. However, they do not have to be taken sequentially. Clearly in the first months a huge study of the literature is necessary, to get acquainted with the tools of Linear Logic, Geometry of Interaction and Optimal Reduction. After this first period the actual research can begin following two parallel tracks, one theoretical and one more practical. At the beginning more emphasis should be on the practical side, to point out if our hypothesis on efficiency are correct. Meanwhile we think that the practical implementation could give us more insights on the theoretical open problems, in particular those of algorithmic nature. In Table \ref{table:schedule} a tentative time schedule is given.
\begin{table}[h]
	\centering
	\begin{tabular}{|c|>{\raggedright}m{10cm}|}
		\hline 
		\textbf{Year} & \textbf{Activities}\tabularnewline
		\hline 
		\hline 
		1 & Literature survey, implementation\tabularnewline
		\hline 
		2 & Collecting results from the implementation, theoretical investigation \tabularnewline
		\hline 
		3 & Further investigation, trying to apply our results to other open theoretical problems and to real programming language implementations, dissertation writing \tabularnewline
		\hline 
	\end{tabular}
	\caption{Tentative project time schedule}
	\label{table:schedule}
\end{table}
\section{Expected Results}
\bibliographystyle{alpha}
\bibliography{project}
\end{document}