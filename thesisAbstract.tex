\documentclass[english]{llncs}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{microtype}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{ifthen}
\renewcommand{\labelitemi}{$\bullet$}

%%% TERMS %%%
\newcommand{\termone}{M}
\newcommand{\termtwo}{N}
\newcommand{\termthree}{L}

%%% REDUCTION RELATIONS %%%
\newcommand{\redbeta}{\longrightarrow_\beta}
\newcommand{\redbetaclo}{\xtwoheadrightarrow{}_\beta}
\newcommand{\red}{\rightarrow}
\newcommand{\redlo}{\longrightarrow_\pslo}
\newcommand{\redri}{\longrightarrow_\psri}
\newcommand{\redbetardx}[1]{\overset{#1}{\longrightarrow_\beta}}
\newcommand{\redbetared}[1]{\overset{#1}{\xtwoheadrightarrow{}_\beta}}
\newcommand{\redlosteps}[1]{\longrightarrow_\pslo^#1}
\newcommand{\redlordx}[1]{\overset{#1}{\longrightarrow}_\pslo}
\newcommand{\redanf}{\underset{\mathsf{ANF}}{\longrightarrow}}
\newcommand{\redbetaanf}{\underset{\beta\mathsf{ANF}}{\longrightarrow}}
\newcommand{\redbetaanfsteps}[1]{\underset{\beta\mathsf{ANF}}{\longrightarrow^{#1}}}
\newcommand{\redbetasteps}[1]{\longrightarrow^{#1}_\beta}
\newcommand{\redbetaanfrdx}[1]{\overset{#1}{\underset{\beta\mathsf{ANF}}{\longrightarrow}}}

%%% STRATEGIES %%%
\newcommand{\psone}{\mathsf{P}}
\newcommand{\mcp}[1]{\mathcal{M}_{#1}}
\newcommand{\density}[1]{{f}_{#1}}
\newcommand{\densityp}[2]{{f}_{#1}^{#2}}
\newcommand{\psuni}{\mathsf{U}}
\newcommand{\pslo}{\mathsf{LO}}
\newcommand{\psri}{\mathsf{RI}}
\newcommand{\psparam}[1]{\mathsf{P}(#1)}
\newcommand{\psparamtwo}[1]{\mathsf{P_2}(#1)}
\newcommand{\psgeo}{\mathsf{G}}
\newcommand{\dist}[1]{\mathsf{Dist}\left(#1\right)}
\newcommand{\supp}[1]{\mathbf{Supp}\left(#1\right)}
\newcommand{\pdist}[1]{\mathsf{PDist}\left(#1\right)}
\newcommand{\derlenght}[1]{\mathsf{dl_\mathcal{#1}}}
\newcommand{\avglenght}[1]{\mathsf{adl_\mathcal{#1}}}

\newboolean{monotonicity}
\setboolean{monotonicity}{true}


\begin{document}
\title{Master Thesis Abstract}
\subtitle{Probabilistic strategies in the $\lambda$-calculus\thanks{Work carried out at Dipartimento di Informatica - Scienza e Ingegneria, Universit√† di Bologna under the supervision of Prof. Ugo Dal Lago.}}
\author{Gabriele Vanoni}
\institute{Politecnico di Milano\\
	\email{gabriele.vanoni@mail.polimi.it}}
\maketitle
There are different possible \emph{strategies} you can follow to evaluate expressions. Some are better than others, and bring you to the result in a lower number of steps. Since programs in pure functional languages are essentially expressions, the problem of defining good strategies is particularly interesting. Finding \emph{minimal} strategies, i.e. strategies that minimize the number of steps, seems even more interesting. However the problem of picking the redex leading to the reduction sequence of least length has been proven undecidable for the $\lambda$-calculus \cite[Section~13.5]{barendregt_lambda_1984}, \emph{the} paradigmatic pure functional language. In the last decades several reduction strategies have been developed. \texttt{Caml} for example comes equipped with a \emph{call-by-value} reduction strategy, while \texttt{Haskell} programs are evaluated with a \emph{call-by-need} strategy. The $\lambda$-calculus is a good abstraction to study reduction mechanisms beacause of his very simple structure. In fact, although \emph{Turing-complete}, it can be seen as a rewriting system \cite{terese_term_2003}, where terms can be formed in only two ways, by abstraction and application, and only one rewriting rule, the $\beta$-rule, is present. Reduction strategies for the $\lambda$-calculus are typically defined according to the position of the contracted \emph{redex} e.g. \emph{leftmost-outermost}, \emph{leftmost-innermost}, \emph{rightmost-innermost}. We show by means of examples the importance of the strategy for performances and termination.
\begin{example}\label{example:canc}
	Let $\bm{\omega}=\lambda x.xx$ and $\bm{\Omega}=\bm{\omega\omega}$. We now consider the reduction of the term $(\lambda x.y)\bm{\Omega}$ according to two different reduction strategies, namely leftmost-outermost ($\pslo$) and rightmost-innermost ($\psri$).
	$$
	(\lambda x.y)\bm{\Omega}\redlo y
	$$
	$$
	(\lambda x.y)\bm{\Omega}\redri(\lambda x.y)\bm{\Omega}\redri(\lambda x.y)\bm{\Omega}\redri\cdots
	$$
	$\bm{\Omega}$ is a looping combinator i.e. a $\lambda$-term which is evaluated to itself. However in $(\lambda x.y)\bm{\Omega}$ the argument $\bm{\Omega}$ is discarded since the function returns the constant $y$. Thus leftmost-outermost (akin to call-by-name in functional programming languages) yields a normal form in one step. Conversely rightmost-innermost (akin to call-by-value) continues to evaluate the argument $(\lambda x.y)\bm{\Omega}$, though it is useless, and rewrites always the same term, yielding to a non-terminating process.
\end{example}
\begin{example}\label{example:copy}
	Let $\bm{I}=\lambda x.x$. We now consider the reduction of the term $(\lambda x.xx)(\bm{II})$, according to $\pslo$ and $\psri$ strategies, as above.
	$$
	(\lambda x.xx)(\bm{II}) \redlo (\bm{II})(\bm{II}) \redlo \bm{I}(\bm{II}) \redlo \bm{II} \redlo \bm{I}
	$$
	$$
	(\lambda x.xx)(\bm{II}) \redri (\lambda x.xx)\bm{I} \redri \bm{II} \redri \bm{I}
	$$
	Here the argument $\bm{II}$ is duplicated and thus it is convenient to reduce it before it is copied, as in righmost-innermost or call-by value. Leftmost-outermost or call-by-name do, indeed, some useless work.
\end{example}
In general, call-by-value is considered more efficent, beacause programs often need to copy their arguments more than once (as in Example \ref{example:copy}). However, as seen in Example \ref{example:canc}, call-by-value is not normalizing. There exist terms which have a normal form but this is not reached using a call-by-value strategy. Instead, a classical result by Curry and Feys \cite{curry_combinatory_1958}, states that reducing a term call-by-name always brings to normal norm, if it exists. Thus, call-by-name is slower, but in a sense, safer. Can we get, in some way, the benefits from both the worlds? 
All reduction strategies for the $\lambda$-calculus present in the literature up to now are \emph{deterministic}, i.e. are (partial) functions from terms to terms. However there is some work on probabilistic rewrite systems \cite{bournez_proving_2005,ferrer_fioriti_probabilistic_2015,avanzini_probabilistic_2018}, in particular regarding termination. But what would happen if the redex to reduce were picked according to some probability distribution? How many steps would a term need to reach a normal form in \emph{average}? In this work in particular we consider a \emph{probabilistic} reduction strategy $\mathsf{P}_\varepsilon$, where the $\pslo$-redex is reduced with probability $\varepsilon$ and the $\psri$-redex with probability $1-\varepsilon$. These are our main results.
\begin{itemize}
	\item $\mathsf{P}_\varepsilon$ is a normalizing strategy. That means that if a term $\termone$ is weakly normalizing, then the expected number of reduction steps from $\termone$ to its normal form with strategy $\mathsf{P}_\varepsilon$ is finite.
	\item $\mathsf{P}_\varepsilon$ is a non-trivial strategy. In fact there exists a class of terms for which $\mathsf{P}_\varepsilon$ is better, in average, than both $\pslo$ and $\psri$.
	\ifthenelse {\boolean{monotonicity}}{\item The function described by the expected number of reduction steps with strategy $\mathsf{P}_\varepsilon$ depending on $\varepsilon$ is monotonically increasing [decreasing] for the sub-calculus $\lambda I$ [$\lambda A$].}{
		\item The function described by the expected number of reduction steps with strategy $\mathsf{P}_\varepsilon$ depending on $\varepsilon$ has minimum for $\varepsilon=0$ [$=1$] for the sub-calculus $\lambda I$ [$\lambda A$].}
\end{itemize}
\bibliographystyle{alpha}
\bibliography{thesisAbstract}
\end{document}